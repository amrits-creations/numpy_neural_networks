{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72243801-924e-4fca-96f1-ae46b66c0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    values = np.unique(y).size\n",
    "    encoded_outputs = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        new_output = [0] * values\n",
    "        new_output[y[i]] = 1\n",
    "        encoded_outputs.append(new_output)\n",
    "\n",
    "    return np.array(encoded_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9169409-b07e-44e2-ae87-07e85bd4553b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 00:49:19.007091: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-07 00:49:19.007794: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-07 00:49:19.117243: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-07 00:49:21.380409: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-07 00:49:21.381032: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape((60000, 784))\n",
    "x_test = x_test.reshape((10000, 784))\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T # Transposed to match the standard dimensions... The training/testing examples should be column vectors.\n",
    "\n",
    "y_train_original = y_train\n",
    "y_test_original = y_test\n",
    "\n",
    "y_train = one_hot_encode(y_train)\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0657b485-983c-4a8b-8b90-8753f55a5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFN:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = []\n",
    "        self.weights.append(0)\n",
    "        # Initialize using He initialization\n",
    "        self.weights.append(np.random.randn(128, 784) * np.sqrt(2. / 784)) \n",
    "        self.weights.append(np.random.randn(16, 128) * np.sqrt(2. / 128)) \n",
    "        self.weights.append(np.random.randn(10, 16) * np.sqrt(2. / 16))\n",
    "\n",
    "\n",
    "        self.biases = []\n",
    "        self.biases.append(0)\n",
    "        self.biases.append(np.zeros((128, 1)))\n",
    "        self.biases.append(np.zeros((16, 1)))\n",
    "        self.biases.append(np.zeros((10, 1)))\n",
    "\n",
    "    def sigmoid(self, x): # unused\n",
    "        return 1 / (1 + np.exp(- x))\n",
    "\n",
    "    def sigmoid_derivative(self, x): #unused\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    def RELU(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def RELU_derivative(self, x):\n",
    "        return (x > 0).astype(int)\n",
    "\n",
    "    def softmax(self, x): \n",
    "        # shift x to be centered around zero for numerical stability, may give inf otherwise\n",
    "        shifted_x = x - np.max(x, axis=0, keepdims=True)\n",
    "        exps = np.exp(shifted_x)\n",
    "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
    "\n",
    "    def softmax_derivative(self):\n",
    "        pass # implemented in the code.\n",
    "\n",
    "    def forward_prop(self, input): # note: input has to be a strict column vector here.\n",
    "        a = []\n",
    "        z = []\n",
    "        \n",
    "        a.append(input)\n",
    "        z.append([0])\n",
    "        z_temp = (self.weights[1] @ a[0]) + self.biases[1] # @ is pretty much just matrix multiplication.\n",
    "        a_temp = self.RELU(z_temp)\n",
    "\n",
    "        z.append(z_temp)\n",
    "        a.append(a_temp)\n",
    "\n",
    "        z_temp = (self.weights[2] @ a[1]) + self.biases[2]\n",
    "        a_temp = self.RELU(z_temp)\n",
    "\n",
    "        z.append(z_temp)\n",
    "        a.append(a_temp)\n",
    "\n",
    "        z_temp = (self.weights[3] @ a[2]) + self.biases[3]\n",
    "        a_temp = self.softmax(z_temp)\n",
    "\n",
    "        z.append(z_temp)\n",
    "        a.append(a_temp)\n",
    "\n",
    "        return z, a\n",
    "\n",
    "    def back_prop(self, z, a, y, alpha): # dX is the derivative of loss wrt X.\n",
    "        dZ3 = a[3] - y # It's a neat calculus trick that skips the jacobian calculation for softmax derivative altogether.\n",
    "        dW3 = dZ3 @ a[2].T\n",
    "        dB3 = dZ3 # Calculating and chaining up the derivatives like we did in the last code...\n",
    "        \n",
    "        dA2 = self.weights[3].T @ dZ3  # column vector\n",
    "        dZ2 = dA2 * self.RELU_derivative(z[2]) # column vector\n",
    "        dW2 = dZ2 @ a[1].T\n",
    "        dB2 = dZ2\n",
    "\n",
    "        dA1 = self.weights[2].T @ dZ2 \n",
    "        dZ1 = dA1 * self.RELU_derivative(z[1])\n",
    "        dW1 = dZ1 @ a[0].T\n",
    "        dB1 = dZ1\n",
    "\n",
    "        # update everything.\n",
    "\n",
    "        self.weights[3] -= alpha * dW3\n",
    "        self.biases[3] -= alpha * dB3\n",
    "\n",
    "        self.weights[2] -= alpha * dW2\n",
    "        self.biases[2] -= alpha * dB2\n",
    "\n",
    "        self.weights[1] -= alpha * dW1\n",
    "        self.biases[1] -= alpha * dB1\n",
    "\n",
    "    def train(self, x_train, y_train, y_train_original, learning_rate = 0.01, epochs = 20):\n",
    "        samples = x_train.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch - {epoch}\")\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for i in range(samples):\n",
    "                x = x_train[:, i].reshape(-1, 1)\n",
    "                y = y_train[:, i].reshape(-1, 1)\n",
    "                # Now we have x and y, single examples as column vectors.\n",
    "    \n",
    "                z, a = self.forward_prop(x)\n",
    "                y_class_index = y_train_original[i]\n",
    "                epsilon = 1e-9 # or else log(0) gives inf.\n",
    "                loss = - np.log(a[3][y_class_index, 0] + epsilon)\n",
    "                self.back_prop(z, a, y, alpha = learning_rate)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "            # calculate epoch loss averaged over epoch\n",
    "            epoch_loss /= samples\n",
    "            print(f\"Epoch loss = {epoch_loss}\\n\")\n",
    "\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9cae72-148f-4678-8d3a-7296813d43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 0\n",
      "Epoch loss = 0.22478799041488176\n",
      "\n",
      "Epoch - 1\n",
      "Epoch loss = 0.10830480374033896\n",
      "\n",
      "Epoch - 2\n",
      "Epoch loss = 0.07860321085063143\n",
      "\n",
      "Epoch - 3\n",
      "Epoch loss = 0.06294502715091953\n",
      "\n",
      "Epoch - 4\n",
      "Epoch loss = 0.053277201644223275\n",
      "\n",
      "Epoch - 5\n",
      "Epoch loss = 0.04763272127417491\n",
      "\n",
      "Epoch - 6\n",
      "Epoch loss = 0.04345994598034939\n",
      "\n",
      "Epoch - 7\n",
      "Epoch loss = 0.038325217962686606\n",
      "\n",
      "Epoch - 8\n",
      "Epoch loss = 0.037497168224951825\n",
      "\n",
      "Epoch - 9\n",
      "Epoch loss = 0.03248101209132064\n",
      "\n",
      "Epoch - 10\n",
      "Epoch loss = 0.0282300614045651\n",
      "\n",
      "Epoch - 11\n",
      "Epoch loss = 0.028483056005214953\n",
      "\n",
      "Epoch - 12\n",
      "Epoch loss = 0.02569521260641412\n",
      "\n",
      "Epoch - 13\n",
      "Epoch loss = 0.026929421590358795\n",
      "\n",
      "Epoch - 14\n",
      "Epoch loss = 0.024921746197188444\n",
      "\n",
      "Epoch - 15\n",
      "Epoch loss = 0.021780927763742363\n",
      "\n",
      "Epoch - 16\n",
      "Epoch loss = 0.02194918497742459\n",
      "\n",
      "Epoch - 17\n",
      "Epoch loss = 0.024217969334076602\n",
      "\n",
      "Epoch - 18\n",
      "Epoch loss = 0.01935406615656663\n",
      "\n",
      "Epoch - 19\n",
      "Epoch loss = 0.022683713267969714\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model = DFN()\n",
    "\n",
    "model.train(x_train, y_train, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e7ca325-2469-421a-bca9-a85b87913d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing\n",
    "\n",
    "def evaluate_model(model, x_test, y_test_original):\n",
    "    print(\"\\nStarting model evaluation on the test set...\")\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    num_samples = x_test.shape[1]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = x_test[:, i].reshape(-1, 1)\n",
    "        true_label = y_test_original[i]\n",
    "\n",
    "        _, a = model.forward_prop(image)\n",
    "        predicted_label = np.argmax(a[3])\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = (correct_predictions / num_samples) * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Correctly classified {correct_predictions} out of {num_samples} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3defd7f0-8c21-48f5-8e20-042edc29d778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model evaluation on the test set...\n",
      "Test Accuracy: 97.34%\n",
      "Correctly classified 9734 out of 10000 samples.\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, x_test, y_test_original) # It generally performs better than the previous model. Let's see if we can make it better..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
