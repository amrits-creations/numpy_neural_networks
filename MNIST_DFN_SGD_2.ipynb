{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "067d8a97-7475-47c9-b44c-07f8fc55eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    values = np.unique(y).size\n",
    "    encoded_outputs = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        new_output = [0] * values\n",
    "        new_output[y[i]] = 1\n",
    "        encoded_outputs.append(new_output)\n",
    "\n",
    "    return np.array(encoded_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe2a33e-c6e9-4f89-b5ec-45cfe3986877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 12:58:35.246172: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-08 12:58:35.264162: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-08 12:58:35.869945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-08 12:58:39.310691: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-08 12:58:39.314111: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape((60000, 784))\n",
    "x_test = x_test.reshape((10000, 784))\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T # Transposed to match the standard dimensions... The training/testing examples should be column vectors.\n",
    "\n",
    "y_train_original = y_train\n",
    "y_test_original = y_test\n",
    "\n",
    "y_train = one_hot_encode(y_train)\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deea1797-ac5d-4be8-bc19-57e302d78538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape # Here, we will be implementing mini batch GD. We need to slice the x train array to get the first n columns where n is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9d5ae2-7f65-40ce-9075-2117e5ad1aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slicing to get the first 10 training examples in x_train:\n",
    "x_train[:, :10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "627e404c-5749-4acf-a74d-7b09d60ea1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shall be using something like this for the forward and backward prop now instead of the single column vectors being used in SGD. Everything else stays the same for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1fc6422-9324-46ce-8cc7-5d83f4487426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFN:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = []\n",
    "        self.weights.append(0)\n",
    "        self.weights.append(np.random.randn(128, 784) * np.sqrt(2. / 784)) \n",
    "        self.weights.append(np.random.randn(16, 128) * np.sqrt(2. / 128)) \n",
    "        self.weights.append(np.random.randn(10, 16) * np.sqrt(2. / 16))\n",
    "\n",
    "\n",
    "        self.biases = []\n",
    "        self.biases.append(0)\n",
    "        self.biases.append(np.zeros((128, 1)))\n",
    "        self.biases.append(np.zeros((16, 1)))\n",
    "        self.biases.append(np.zeros((10, 1)))\n",
    "        \n",
    "        # Here, I have removed the sigmoid and it's derivative methods altogether. \n",
    "\n",
    "    def RELU(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def RELU_derivative(self, x):\n",
    "        return (x > 0).astype(int)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        shifted_x = x - np.max(x, axis=0, keepdims=True)\n",
    "        exps = np.exp(shifted_x)\n",
    "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
    "\n",
    "    def softmax_derivative(self):\n",
    "        pass # implemented in the code.\n",
    "\n",
    "    def forward_prop(self, input): # This method can actually process mini batches already as it uses matrix multiplication.\n",
    "        a = []\n",
    "        z = []\n",
    "        \n",
    "        a.append(input)\n",
    "        z.append([0])\n",
    "        z_temp = (self.weights[1] @ a[0]) + self.biases[1]\n",
    "        a_temp = self.RELU(z_temp)\n",
    "\n",
    "        z.append(z_temp)\n",
    "        a.append(a_temp)\n",
    "\n",
    "        z_temp = (self.weights[2] @ a[1]) + self.biases[2]\n",
    "        a_temp = self.RELU(z_temp)\n",
    "\n",
    "        z.append(z_temp)\n",
    "        a.append(a_temp)\n",
    "\n",
    "        z_temp = (self.weights[3] @ a[2]) + self.biases[3]\n",
    "        a_temp = self.softmax(z_temp)\n",
    "\n",
    "        z.append(z_temp)\n",
    "        a.append(a_temp)\n",
    "\n",
    "        return z, a\n",
    "\n",
    "    def back_prop(self, z, a, y, alpha):\n",
    "        m = a[0].shape[1]\n",
    "        \n",
    "        dZ3 = a[3] - y \n",
    "        dW3 = (1 / m) * (dZ3 @ a[2].T)\n",
    "        dB3 = (1 / m) * np.sum(dZ3, axis = 1, keepdims = True)\n",
    "        \n",
    "        dA2 = self.weights[3].T @ dZ3\n",
    "        dZ2 = dA2 * self.RELU_derivative(z[2])\n",
    "        dW2 = (1 / m) * (dZ2 @ a[1].T)\n",
    "        dB2 = (1 / m) * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "\n",
    "        dA1 = self.weights[2].T @ dZ2 \n",
    "        dZ1 = dA1 * self.RELU_derivative(z[1])\n",
    "        dW1 = (1 / m) * (dZ1 @ a[0].T)\n",
    "        dB1 = (1 / m) * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "\n",
    "        self.weights[3] -= alpha * dW3\n",
    "        self.biases[3] -= alpha * dB3\n",
    "\n",
    "        self.weights[2] -= alpha * dW2\n",
    "        self.biases[2] -= alpha * dB2\n",
    "\n",
    "        self.weights[1] -= alpha * dW1\n",
    "        self.biases[1] -= alpha * dB1\n",
    "\n",
    "    def train(self, x_train, y_train, y_train_original, learning_rate = 0.1, epochs = 20, batch_size = 64):\n",
    "        samples = x_train.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch - {epoch}\")\n",
    "\n",
    "            # Here, we shuffle the indices at every epoch.\n",
    "            shuffled_index = np.random.permutation(samples)\n",
    "            x_train_shuffled = x_train[:, shuffled_index]\n",
    "            y_train_shuffled = y_train[:, shuffled_index]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for i in range(0, samples, batch_size):\n",
    "                # Add a variable to hold the index of the end of the current batch, and use it in the indexing \n",
    "                # to extract a mini batch of columns instead of a single column.\n",
    "                end = i + batch_size\n",
    "                x_batch = x_train_shuffled[:, i:end]\n",
    "                y_batch = y_train_shuffled[:, i:end]\n",
    "    \n",
    "                z, a = self.forward_prop(x_batch)\n",
    "                self.back_prop(z, a, y_batch, alpha = learning_rate)\n",
    "                \n",
    "                y_class_indices = np.argmax(y_batch, axis = 0)\n",
    "                epsilon = 1e-9 # or else log(0) gives inf.\n",
    "                log_probs = - np.log(a[3][y_class_indices, np.arange(y_batch.shape[1])] + epsilon)\n",
    "                loss = np.sum(log_probs)\n",
    "                self.back_prop(z, a, y_batch, alpha = learning_rate)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "            # calculate epoch loss averaged over epoch\n",
    "            epoch_loss /= samples\n",
    "            print(f\"Epoch loss = {epoch_loss}\\n\")\n",
    "\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b59b295-e71c-40fa-b280-1a67dffb6df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 0\n",
      "Epoch loss = 0.3388507579899295\n",
      "\n",
      "Epoch - 1\n",
      "Epoch loss = 0.1274720884221863\n",
      "\n",
      "Epoch - 2\n",
      "Epoch loss = 0.09281831736342164\n",
      "\n",
      "Epoch - 3\n",
      "Epoch loss = 0.07060219329596319\n",
      "\n",
      "Epoch - 4\n",
      "Epoch loss = 0.058258079561518426\n",
      "\n",
      "Epoch - 5\n",
      "Epoch loss = 0.048306090979272714\n",
      "\n",
      "Epoch - 6\n",
      "Epoch loss = 0.039006205167396564\n",
      "\n",
      "Epoch - 7\n",
      "Epoch loss = 0.0328506230055295\n",
      "\n",
      "Epoch - 8\n",
      "Epoch loss = 0.02824125850103934\n",
      "\n",
      "Epoch - 9\n",
      "Epoch loss = 0.0260483820433257\n",
      "\n",
      "Epoch - 10\n",
      "Epoch loss = 0.020112576851689285\n",
      "\n",
      "Epoch - 11\n",
      "Epoch loss = 0.017485025199045635\n",
      "\n",
      "Epoch - 12\n",
      "Epoch loss = 0.01423745758475049\n",
      "\n",
      "Epoch - 13\n",
      "Epoch loss = 0.011188672755925482\n",
      "\n",
      "Epoch - 14\n",
      "Epoch loss = 0.00749872002048807\n",
      "\n",
      "Epoch - 15\n",
      "Epoch loss = 0.006553118305421625\n",
      "\n",
      "Epoch - 16\n",
      "Epoch loss = 0.005117532939057155\n",
      "\n",
      "Epoch - 17\n",
      "Epoch loss = 0.0031538335169055986\n",
      "\n",
      "Epoch - 18\n",
      "Epoch loss = 0.001492761088867562\n",
      "\n",
      "Epoch - 19\n",
      "Epoch loss = 0.0008482850605995106\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model = DFN()\n",
    "\n",
    "model.train(x_train, y_train, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f8ab3cc-093d-44bf-9814-b8979d18153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing\n",
    "\n",
    "def evaluate_model(model, x_test, y_test_original):\n",
    "    print(\"\\nStarting model evaluation on the test set...\")\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    num_samples = x_test.shape[1]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = x_test[:, i].reshape(-1, 1)\n",
    "        true_label = y_test_original[i]\n",
    "\n",
    "        _, a = model.forward_prop(image)\n",
    "        predicted_label = np.argmax(a[3])\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = (correct_predictions / num_samples) * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Correctly classified {correct_predictions} out of {num_samples} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50203aed-dd85-4ff6-93a2-40ddb7c3b663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model evaluation on the test set...\n",
      "Test Accuracy: 97.97%\n",
      "Correctly classified 9797 out of 10000 samples.\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, x_test, y_test_original) # It generally performs better than the previous model. Let's see if we can make it better..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
